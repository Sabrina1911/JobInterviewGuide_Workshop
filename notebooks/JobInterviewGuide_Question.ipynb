{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3829938",
   "metadata": {},
   "source": [
    "\n",
    "# JobInterviewGuide_Workshop — Junior ML Specialist Prep\n",
    "\n",
    "This notebook targets the exact areas from your quiz that need reinforcement.  \n",
    "Each section includes a **concise concept recap** (Markdown) and **scaffolded code cells** (with `TODO` prompts).\n",
    "\n",
    "**Focus areas from your quiz:**\n",
    "1. Logistic Regression probabilities (sigmoid) ✅ *what the output means*  \n",
    "2. Cross-Entropy (Log-Loss) vs Regularization ✅ *don’t confuse the roles*  \n",
    "3. Decision Trees — what **leaf nodes** represent  \n",
    "4. Classification Metrics — know which **apply to classification** vs **regression**  \n",
    "5. Parametric vs Non-Parametric models — definitions + tiny experiment  \n",
    "6. Feature Engineering — the real goal (not random features)  \n",
    "7. Train/Validation/Test — the **validation** set’s purpose  \n",
    "8. Gradient Descent — optimization, not scaling\n",
    "\n",
    "> Tip: Run each code cell, fill the TODOs, and keep short notes below each section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e05997",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Logistic Regression & the Sigmoid Output (Probability)\n",
    "\n",
    "**Key point:** The sigmoid function \\( \\sigma(z) = 1/(1+e^{-z}) \\) maps any real number to a **probability** in \\([0,1]\\).  \n",
    "In logistic regression, \\(z = w^T x + b\\). The model predicts **P(y=1|x)**.\n",
    "\n",
    "**Why it matters:** Your answer mixed this with residuals. Residuals are a regression concept; **sigmoid outputs probabilities**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbab871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Compute sigmoid probabilities for a set of log-odds (z values)\n",
    "import math\n",
    "\n",
    "z_values = [-4, -1, 0, 1, 3]\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + math.exp(-z))\n",
    "\n",
    "probs = [sigmoid(z) for z in z_values]\n",
    "print(\"z:\", z_values)\n",
    "print(\"sigmoid(z):\", [round(p, 4) for p in probs])\n",
    "\n",
    "# TODO: Given a probability threshold=0.5, convert these to class predictions (0/1).\n",
    "threshold = 0.5\n",
    "preds = [1 if p >= threshold else 0 for p in probs]\n",
    "print(\"class predictions @0.5:\", preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc274e9",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Cross-Entropy (Log-Loss) vs Regularization\n",
    "\n",
    "**Cross-Entropy (Log-Loss)** measures how close predicted probabilities are to true labels:  \n",
    "\\[ \\mathcal{L}_{CE} = -\\frac{1}{N}\\sum_i \\big( y_i\\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i) \\big) \\]\n",
    "\n",
    "**Regularization (L1/L2)** penalizes large weights to control complexity:  \n",
    "\\( \\mathcal{L}_{total} = \\mathcal{L}_{CE} + \\lambda \\cdot \\text{Penalty}(w) \\)\n",
    "\n",
    "> You chose \"penalize large coefficients\" for cross-entropy. That’s **regularization**, not CE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa92a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement binary cross-entropy for a tiny example\n",
    "import math\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "y_hat =  [0.9, 0.2, 0.6, 0.8, 0.1]  # predicted probabilities\n",
    "\n",
    "def binary_cross_entropy(y, p):\n",
    "    eps = 1e-12\n",
    "    loss = 0.0\n",
    "    for yt, pt in zip(y, p):\n",
    "        pt = min(max(pt, eps), 1.0 - eps)\n",
    "        loss += -(yt*math.log(pt) + (1-yt)*math.log(1-pt))\n",
    "    return loss/len(y)\n",
    "\n",
    "ce = binary_cross_entropy(y_true, y_hat)\n",
    "print(\"Cross-Entropy Loss:\", round(ce, 4))\n",
    "\n",
    "# TODO: Add simple L2 regularization term to the total loss for weights w\n",
    "w = [0.8, -0.3, 0.5]\n",
    "lam = 0.1\n",
    "l2 = lam * sum(wi*wi for wi in w)\n",
    "total = ce + l2\n",
    "print(\"L2 penalty:\", round(l2, 4), \"  Total loss:\", round(total, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d0c54",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Decision Trees — Leaf Nodes = Final Predictions\n",
    "\n",
    "A **leaf node** is where splitting stops; it contains the **final prediction** (class label or mean value).  \n",
    "> You selected \"a split point\" — that’s an **internal node**, not a leaf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b03f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Train a tiny decision tree and inspect leaf predictions\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "X = [[0],[1],[2],[3],[4],[5]]\n",
    "y = [0,0,0,1,1,1]\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X,y)\n",
    "\n",
    "print(export_text(clf, feature_names=[\"x\"]))\n",
    "\n",
    "# TODO: Predict for x = 1.5 and x = 3.2, and explain which leaf each ends in.\n",
    "print(\"Pred(1.5) =\", clf.predict([[1.5]])[0])\n",
    "print(\"Pred(3.2) =\", clf.predict([[3.2]])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50802930",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Classification Metrics vs Regression Metrics\n",
    "\n",
    "**Classification:** accuracy, precision, recall, F1, confusion matrix.  \n",
    "**Regression:** R², MSE, RMSE, MAE.  \n",
    "> You picked F1 as \"not used for classification\" — the true mismatch is **R²** (regression only).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Compute accuracy, precision, recall, F1 on a small example\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "y_true = [1,0,1,1,0,1]\n",
    "y_pred = [1,0,0,1,0,1]\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"Accuracy:\", round(accuracy_score(y_true,y_pred),3))\n",
    "print(\"Precision:\", round(precision_score(y_true,y_pred),3))\n",
    "print(\"Recall:\", round(recall_score(y_true,y_pred),3))\n",
    "print(\"F1:\", round(f1_score(y_true,y_pred),3))\n",
    "\n",
    "# QUESTION: Which metric belongs to regression only?  -> R^2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107772b9",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Parametric vs Non-Parametric — Mini Experiment\n",
    "\n",
    "- **Parametric**: fixed number of parameters (e.g., Linear Regression).  \n",
    "- **Non-Parametric**: model complexity grows with data (e.g., KNN, Decision Trees).  \n",
    "> You selected that both require linear relationships — **not true**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Compare Linear Regression (parametric) vs KNN Regressor (non-parametric) on a non-linear function\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X = np.linspace(-3, 3, 60).reshape(-1,1)\n",
    "y = np.sin(X).ravel() + 0.1*rng.randn(60)\n",
    "\n",
    "# Parametric\n",
    "lin = LinearRegression().fit(X,y)\n",
    "mse_lin = mean_squared_error(y, lin.predict(X))\n",
    "\n",
    "# Non-Parametric\n",
    "knn = KNeighborsRegressor(n_neighbors=5).fit(X,y)\n",
    "mse_knn = mean_squared_error(y, knn.predict(X))\n",
    "\n",
    "print(\"MSE Linear Regression (parametric):\", round(mse_lin,4))\n",
    "print(\"MSE KNN Regressor (non-parametric):\", round(mse_knn,4))\n",
    "\n",
    "# QUESTION: Which handles the non-linear pattern better here? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db54ea8",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Feature Engineering — Purpose\n",
    "\n",
    "**Goal:** Transform raw data into **meaningful** features that improve signal-to-noise and model performance.  \n",
    "It is **not** about randomly adding variables.\n",
    "\n",
    "Examples: scaling, encoding, interaction terms, domain-specific aggregates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Demonstrate a simple feature transform that improves separability\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.uniform(-2,2,(300,1))\n",
    "y = (X[:,0]**2 + rng.normal(0,0.3,300) > 1.0).astype(int)\n",
    "\n",
    "# Baseline: raw feature\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "lr = LogisticRegression().fit(X_tr, y_tr)\n",
    "base_acc = accuracy_score(y_te, lr.predict(X_te))\n",
    "\n",
    "# Add polynomial feature\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "X2_tr, X2_te, y_tr, y_te = train_test_split(X2, y, test_size=0.3, random_state=0)\n",
    "lr2 = LogisticRegression().fit(X2_tr, y_tr)\n",
    "poly_acc = accuracy_score(y_te, lr2.predict(X2_te))\n",
    "\n",
    "print(\"Baseline acc (raw):\", round(base_acc,3))\n",
    "print(\"With polynomial feature acc:\", round(poly_acc,3))\n",
    "\n",
    "# QUESTION: Which feature set performs better? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224708ff",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Train / Validation / Test — Role of Validation\n",
    "\n",
    "- **Train:** fit model parameters  \n",
    "- **Validation:** **tune hyperparameters**, monitor overfitting, pick the best model  \n",
    "- **Test:** final unbiased evaluation\n",
    "\n",
    "> You selected \"balance data\" for validation — its role is **model selection & tuning**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e627c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Show how validation affects hyperparameter selection with a simple grid search\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = make_classification(n_samples=400, n_features=6, n_informative=4, random_state=0)\n",
    "param_grid = {\"n_neighbors\":[1,3,5,7,9]}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=5, scoring=\"accuracy\")\n",
    "grid.fit(X,y)\n",
    "\n",
    "print(\"Best params via CV (validation):\", grid.best_params_)\n",
    "print(\"Best CV accuracy:\", round(grid.best_score_,3))\n",
    "\n",
    "# TODO: Explain why we don't use the test set during this process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc4ab6",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Gradient Descent — Optimization\n",
    "\n",
    "**Definition:** Iteratively updates parameters in the direction of **negative gradient** to **minimize loss**.  \n",
    "Common for linear/logistic regression and neural nets.\n",
    "\n",
    "> You chose a scaling method — but GD is an **optimizer**, not a preprocessor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ee095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement gradient descent to fit y = wx + b on synthetic data\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = rng.rand(200,1)\n",
    "true_w, true_b = 2.0, -0.5\n",
    "y = true_w*X[:,0] + true_b + rng.normal(0,0.05,200)\n",
    "\n",
    "w, b = 0.0, 0.0\n",
    "lr = 0.5\n",
    "for step in range(200):\n",
    "    y_hat = w*X[:,0] + b\n",
    "    # gradients of MSE loss\n",
    "    dw = (2/len(X)) * np.sum((y_hat - y) * X[:,0])\n",
    "    db = (2/len(X)) * np.sum((y_hat - y))\n",
    "    w -= lr*dw\n",
    "    b -= lr*db\n",
    "    if step % 50 == 0:\n",
    "        mse = np.mean((y_hat - y)**2)\n",
    "        print(f\"step={step:3d}  w={w:.3f}  b={b:.3f}  MSE={mse:.4f}\")\n",
    "\n",
    "print(\"Estimated w,b:\", round(w,3), round(b,3), \"  (true:\", true_w, true_b, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e347c6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ✅ How to Use This Notebook\n",
    "- Work through sections in order.\n",
    "- Replace `TODO` stubs with your own code/notes.\n",
    "- If something feels shaky, re-run the earlier study materials (class notebooks) for that topic.\n",
    "\n",
    "When you finish, **export to PDF** and keep it as your *Interview Readiness Workbook*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
