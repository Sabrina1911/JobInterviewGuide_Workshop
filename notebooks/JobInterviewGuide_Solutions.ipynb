{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235d8358",
   "metadata": {},
   "source": [
    "\n",
    "# JobInterviewGuide_Workshop — **Solutions Key**\n",
    "\n",
    "This notebook provides worked solutions and example outputs corresponding to each section of the workshop.\n",
    "Use it to check your work after attempting the exercises yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b78b4",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Logistic Regression & the Sigmoid Output — Solution\n",
    "\n",
    "Sigmoid outputs **probabilities**. Thresholding at 0.5 yields class predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "z_values = [-4, -1, 0, 1, 3]\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + math.exp(-z))\n",
    "probs = [sigmoid(z) for z in z_values]\n",
    "threshold = 0.5\n",
    "preds = [1 if p >= threshold else 0 for p in probs]\n",
    "print(\"z:\", z_values)\n",
    "print(\"sigmoid(z):\", [round(p,4) for p in probs])\n",
    "print(\"class predictions @0.5:\", preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c2ade",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Cross-Entropy (Log-Loss) vs Regularization — Solution\n",
    "\n",
    "Cross-entropy measures probability fit; L2 adds a penalty on large weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5333076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "y_hat  = [0.9, 0.2, 0.6, 0.8, 0.1]\n",
    "def binary_cross_entropy(y, p):\n",
    "    eps = 1e-12\n",
    "    loss = 0.0\n",
    "    for yt, pt in zip(y, p):\n",
    "        pt = min(max(pt, eps), 1.0 - eps)\n",
    "        loss += -(yt*math.log(pt) + (1-yt)*math.log(1-pt))\n",
    "    return loss/len(y)\n",
    "ce = binary_cross_entropy(y_true, y_hat)\n",
    "w = [0.8, -0.3, 0.5]\n",
    "lam = 0.1\n",
    "l2 = lam * sum(wi*wi for wi in w)\n",
    "total = ce + l2\n",
    "print(\"Cross-Entropy Loss:\", round(ce, 4))\n",
    "print(\"L2 penalty:\", round(l2, 4))\n",
    "print(\"Total (CE + L2):\", round(total, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895ba50",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Decision Trees — Leaf Nodes = Final Predictions — Solution\n",
    "\n",
    "Leaf nodes store the final predicted class (or mean for regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddfe19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "X = [[0],[1],[2],[3],[4],[5]]\n",
    "y = [0,0,0,1,1,1]\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X,y)\n",
    "print(export_text(clf, feature_names=[\"x\"]))\n",
    "print(\"Pred(1.5) =\", clf.predict([[1.5]])[0])\n",
    "print(\"Pred(3.2) =\", clf.predict([[3.2]])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50e314",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Classification Metrics vs Regression Metrics — Solution\n",
    "\n",
    "F1 is for classification; **R²** is for regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ea43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "y_true = [1,0,1,1,0,1]\n",
    "y_pred = [1,0,0,1,0,1]\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"Accuracy:\", round(accuracy_score(y_true,y_pred),3))\n",
    "print(\"Precision:\", round(precision_score(y_true,y_pred),3))\n",
    "print(\"Recall:\", round(recall_score(y_true,y_pred),3))\n",
    "print(\"F1:\", round(f1_score(y_true,y_pred),3))\n",
    "print(\"\\nNote: R^2 is a regression metric, not shown here.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7715380",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Parametric vs Non-Parametric — Mini Experiment — Solution\n",
    "\n",
    "On a non-linear signal, non-parametric KNN typically fits better than linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b0581",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X = np.linspace(-3, 3, 60).reshape(-1,1)\n",
    "y = np.sin(X).ravel() + 0.1*rng.randn(60)\n",
    "\n",
    "lin = LinearRegression().fit(X,y)\n",
    "mse_lin = mean_squared_error(y, lin.predict(X))\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5).fit(X,y)\n",
    "mse_knn = mean_squared_error(y, knn.predict(X))\n",
    "\n",
    "print(\"MSE Linear Regression (parametric):\", round(mse_lin,4))\n",
    "print(\"MSE KNN Regressor (non-parametric):\", round(mse_knn,4))\n",
    "print(\"Observation: Lower MSE indicates better fit; KNN often wins on non-linear patterns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bbb0cf",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Feature Engineering — Purpose — Solution\n",
    "\n",
    "Adding informative transforms (e.g., polynomial features) can improve separability and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c929a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.uniform(-2,2,(300,1))\n",
    "y = (X[:,0]**2 + rng.normal(0,0.3,300) > 1.0).astype(int)\n",
    "\n",
    "# Baseline\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "lr = LogisticRegression().fit(X_tr, y_tr)\n",
    "base_acc = accuracy_score(y_te, lr.predict(X_te))\n",
    "\n",
    "# With polynomial feature\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "X2_tr, X2_te, y_tr, y_te = train_test_split(X2, y, test_size=0.3, random_state=0)\n",
    "lr2 = LogisticRegression().fit(X2_tr, y_tr)\n",
    "poly_acc = accuracy_score(y_te, lr2.predict(X2_te))\n",
    "\n",
    "print(\"Baseline acc (raw):\", round(base_acc,3))\n",
    "print(\"With polynomial feature acc:\", round(poly_acc,3))\n",
    "print(\"Observation: Higher accuracy with engineered features indicates improved separability.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653eb45",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Train / Validation / Test — Role of Validation — Solution\n",
    "\n",
    "Validation is used for **hyperparameter tuning** (e.g., via cross-validation) before the final test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504863ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = make_classification(n_samples=400, n_features=6, n_informative=4, random_state=0)\n",
    "param_grid = {\"n_neighbors\":[1,3,5,7,9]}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=5, scoring=\"accuracy\")\n",
    "grid.fit(X,y)\n",
    "print(\"Best params via CV (validation):\", grid.best_params_)\n",
    "print(\"Best CV accuracy:\", round(grid.best_score_,3))\n",
    "print(\"Note: The test set should only be used once for final, unbiased evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3e9d2",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Gradient Descent — Optimization — Solution\n",
    "\n",
    "We minimize MSE for y = wx + b with vanilla GD and converge near the true parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012adc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(1)\n",
    "X = rng.rand(200,1)\n",
    "true_w, true_b = 2.0, -0.5\n",
    "y = true_w*X[:,0] + true_b + rng.normal(0,0.05,200)\n",
    "\n",
    "w, b = 0.0, 0.0\n",
    "lr = 0.5\n",
    "for step in range(200):\n",
    "    y_hat = w*X[:,0] + b\n",
    "    dw = (2/len(X)) * np.sum((y_hat - y) * X[:,0])\n",
    "    db = (2/len(X)) * np.sum((y_hat - y))\n",
    "    w -= lr*dw\n",
    "    b -= lr*db\n",
    "    if step % 50 == 0:\n",
    "        mse = np.mean((y_hat - y)**2)\n",
    "        print(f\"step={step:3d}  w={w:.3f}  b={b:.3f}  MSE={mse:.4f}\")\n",
    "print(\"Estimated w,b:\", round(w,3), round(b,3), \"  (true:\", true_w, true_b, \")\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
