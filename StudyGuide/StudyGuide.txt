Study Guide — Junior Machine Learning Specialist

This guide summarizes the essential algorithms and concepts covered in the workshop materials.

1. Supervised vs. Unsupervised Learning Algorithms
   - Supervised learning uses labeled data to train models (e.g., regression, classification).
   - Unsupervised learning uses unlabeled data to find hidden patterns (e.g., clustering, PCA).

2. Dependent vs. Independent Variables
   - Dependent variables (targets) are predicted from independent variables (features).
   - Understanding this relationship forms the basis for regression and classification models.

3. Training vs. Testing Data: Train–Validation–Test Split
   - Data is split into three parts: training (to fit the model), validation (to tune hyperparameters), and testing (to evaluate final performance).

4. Linear Regression: Residuals, Linearization
   - Linear regression fits a line to minimize the residuals (differences between observed and predicted values).
   - Linearization transforms non-linear relationships into linear ones for modeling.

5. Regression Analysis: Parametric and Non-Parametric Models, R-Squared and MSE
   - Parametric models (e.g., Linear Regression) assume a fixed function form.
   - Non-parametric models (e.g., KNN, Decision Trees) adapt to data flexibly.
   - R² measures explained variance; MSE measures average squared error.

6. Logistic Regression: Intercept, Slope, Cross-Entropy
   - Logistic regression predicts probabilities via the sigmoid function.
   - Intercept and slope determine the decision boundary.
   - Cross-entropy (log-loss) evaluates how well predicted probabilities match actual labels.

7. K-Nearest Neighbors Algorithm: Hyperparameters
   - KNN classifies based on the majority class of k nearest data points.
   - The main hyperparameter is k, which controls model flexibility and bias-variance tradeoff.

8. Decision Trees (Self-Study): Leaf Nodes and Predictions
   - Decision Trees split data recursively based on feature thresholds.
   - Leaf nodes represent final predictions (class or mean value).
   - Common improvements include pruning and ensemble methods (Random Forests, Gradient Boosting).

Each topic connects to practical exercises in the provided Jupyter notebooks and demonstrates both theory and application in real-world ML workflows.
