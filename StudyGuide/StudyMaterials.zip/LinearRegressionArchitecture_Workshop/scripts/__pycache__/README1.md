# Predictive Maintenance â€“ Orchestration (Lab 1 Refactor)

Object-Oriented, modular refactor of the Streaming Predictive Maintenance lab.  
Single CLI entry point (`Orchestrator_main.py`) orchestrates: **DataExtractionAnalysis â†’ DataPreparation â†’ ModelSelection â†’ ModelTraining â†’ ModelEvaluationValidation â†’ TrainedMLModel â†’ Reporting**.  
Includes rotating logs, artifacts/versioning, and a playback utility that mirrors the final notebook â€œsummary/overlayâ€ views.

---

## 1) Repo structure

```
.
â”œâ”€ .env
â”œâ”€ .venv/                     # (local) Python virtual environment
â”œâ”€ artifacts/                 # saved models, thresholds, plots, PDF reports
â”œâ”€ data/
â”‚  â”œâ”€ raw/
â”‚  â”‚  â”œâ”€ metadata_regen_clean.csv
â”‚  â”‚  â”œâ”€ metadata_wilk_aligned.csv
â”‚  â”‚  â”œâ”€ metadata_wilk_clean.csv
â”‚  â”‚  â””â”€ RMBR4-2_export_test.csv
â”‚  â”œâ”€ interim/
â”‚  â””â”€ processed/
â”œâ”€ logs/                      # rotating file logs
â”œâ”€ reports/                   # generated PDFs/PNGs (also copied to artifacts/)
â”œâ”€ DataExtractionAnalysis/
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ dataextractionanalysis.py
â”œâ”€ DataPreparation/
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ datapreparation.py
â”œâ”€ ModelSelection/
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ modelselection.py
â”œâ”€ ModelTraining/
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ modeltraining.py
â”œâ”€ ModelEvaluationValidation/
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ modelevaluationvalidation.py
â”œâ”€ TrainedMLModel/
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ trainedmlmodel.py
â”œâ”€ scripts/
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ playback.py
â”œâ”€ Orchestrator_main.py       # CLI entry point (main)
â””â”€ requirements.txt
```

---

## 2) Quickstart

### A) Create & activate a virtualenv

**Windows (PowerShell):**
```powershell
py -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

**macOS / Linux:**
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

---

### B) Configure environment

Create **.env** at repo root:

```env
# Database
DATABASE_URL=postgresql+psycopg2://<user>:<pass>@<host>/<db>
TRAINING_TABLE=readings_fact_csv

# Optional: MLflow
MLFLOW_TRACKING_URI=
MLFLOW_EXPERIMENT_NAME=PredMaint_Lab1

# Defaults (can be overridden by CLI/YAML)
CADENCE_SECONDS=1.0
```

---

### C) Run with YAML (recommended)

Weâ€™ve included a **pipeline.yaml** at the repo root. It captures all defaults (train table, test CSV, cadence, thresholds, plotting, playback settings, etc.).  

**Windows (PowerShell):**
```powershell
python .\Orchestrator_main.py train --config pipeline.yaml
```

**macOS / Linux:**
```bash
python Orchestrator_main.py train --config pipeline.yaml
```

This will:  
- Pull TRAIN data from DB, align TEST CSV,  
- Train per-axis models, discover thresholds,  
- Save models + thresholds in `artifacts/`,  
- Generate `reports/summary.pdf`,  
- Log events/errors in `logs/app.log`.  

---

### D) Override YAML if needed

You can still override any setting via CLI flags. CLI > YAML > .env in priority.  

Example: same run, but override cadence and plotting:

```powershell
python .\Orchestrator_main.py train --config pipeline.yaml --cadence_s 0.5 --plot overlays
```

---

## 3) CLI â€“ main flows

All commands are executed from the repo root.

### 3.1 Train (DB + CSV)

Pull TRAIN from DB, align TEST CSV to TRAIN, select model per axis, train, compute residual thresholds (IQR-based with dwell T), and generate plots + PDF report.

**Windows (PowerShell):**
```powershell
python .\Orchestrator_main.py train `
  --train_table readings_fact_csv `
  --test_csv .\dataaw\metadata_wilk_clean.csv `
  --cadence_s 1.0 --iqr_k 1.5 --t_mult 3 --plot both
```

**macOS / Linux:**
```bash
python Orchestrator_main.py train   --train_table readings_fact_csv   --test_csv ./data/raw/metadata_wilk_clean.csv   --cadence_s 1.0 --iqr_k 1.5 --t_mult 3 --plot both
```

**Outputs** (written under `artifacts/` and `reports/`):
- `artifacts/models/*.joblib` â€“ per-axis model bundle
- `artifacts/thresholds.csv` â€“ per-axis residual thresholds (`minC,maxC,T_sec`)
- `reports/summary.pdf` â€“ overlay grid + decluttered summary (matches notebook)
- `logs/app.log` â€“ rotating logs

---

### 3.2 Evaluate

Re-uses saved models/thresholds to score the specified TEST CSV, detect events, and produce overlays.

```powershell
python .\Orchestrator_main.py evaluate `
  --train_table readings_fact_csv `
  --test_csv .\dataaw\metadata_wilk_clean.csv `
  --cadence_s 1.0 --plot overlays
```

**YAML-driven:**
```powershell
python .\Orchestrator_main.py evaluate --config pipeline.yaml
```
```bash
python Orchestrator_main.py evaluate --config pipeline.yaml
```

---

### 3.3 Playback (sliding window, â€œCell 19.0 feelâ€)

Visual, time-sliding stacked plot with ALERT/ERROR spans, top-K axes, tick/dwell windowing.

> **Important default fix applied:** the `--artifacts` default should be **project root `.`** (so the loader finds `./artifacts/...`). If you see `.../artifacts/artifacts/...`, pass `--artifacts .` explicitly.

```powershell
python .\Orchestrator_main.py playback `
  --meta .\dataaw\metadata_wilk_clean.csv `
  --artifacts . `
  --tick 2 --window 45 --stacked 1 --topk 8
```

**YAML-driven:**
```powershell
python .\Orchestrator_main.py playback --config pipeline.yaml
```
```bash
python Orchestrator_main.py playback --config pipeline.yaml
```

Key flags:
- `--tick`: seconds per animation tick (or step)
- `--window`: sliding window width (seconds)
- `--stacked 1`: stacked axes (use 0 for separate)
- `--topk 8`: plot the noisiest K axes to reduce clutter
- `--zmode 0|1`: toggle z-score mode vs current mode

---

## 4) Demo Outputs (Sample Artifacts & Reports)

When you run with `pipeline.yaml`, the system produces **artifacts, plots, and logs** in predictable locations. Hereâ€™s where to look:

### Reports
- ğŸ“„ **`reports/summary.pdf`**  
  Final decluttered summary: regression overlays, ALERT/ERROR spans, residual plots.  
  â†’ *Matches the final notebook cells.*  

- ğŸ–¼ï¸ **`reports/plots/*.png`**  
  Individual axis overlays and diagnostic plots (optional).  

### Artifacts
- ğŸ“‚ **`artifacts/models/*.joblib`**  
  Saved per-axis trained models (linear/mean).  

- ğŸ“Š **`artifacts/thresholds.csv`**  
  Residual thresholds per axis (`minC, maxC, T_sec`).  

- ğŸ“‘ **`artifacts/events_YYYYMMDD_HHMMSS.csv`**  
  Event log: axis, label (ALERT/ERROR), start/end timestamps.  

### Logs
- ğŸ“œ **`logs/app.log`**  
  Rotating application log with run config, DB pulls, training stats, detection summaries.  

### Playback (visual demo)
Running:
```powershell
python .\Orchestrator_main.py playback --config pipeline.yaml
```
will open an **animated sliding window** showing:  
- Time-based stacked signals  
- ALERT/ERROR spans shaded  
- Top-K axes only (to reduce clutter)  
This visually replicates **Notebook Cell 19.0**.

**Example screenshot references (optional):**
```markdown
![Summary PDF Preview](reports/summary_preview.png)
![Playback Window](reports/playback_preview.png)
```
(Export a frame from playback or the first page of `summary.pdf` as PNG and drop into `reports/`.)

---

## 5) What this repo satisfies (rubric mapping)

**Project Setup (1 pt)**  
- README (this file), `requirements.txt`, `data/` present.  
- CLI entry point with `if __name__ == "__main__":` in `Orchestrator_main.py`.  
- Final PDF report saved under `reports/`.

**Database Integration (1.5 pt)**  
- SQLAlchemy connection to Neon (or any Postgres URI in `.env`).  
- TRAIN table read from `TRAINING_TABLE`.  
- CSVâ†’DB helper provided for ingestion.

**Streaming Simulation (1 pt)**  
- Cadence/â€œgapâ€ checks in DataPreparation.  
- `scripts/playback.py` mimics time-windowed streaming visually.

**Regression & Residual Analysis (2 pt)**  
- Per-axis linear/mean models; residuals computed and logged.  
- Overlay charts include regression and residual spans.

**Threshold Discovery & Justification (2 pt)**  
- Robust IQR-based bands on residuals with two-sided limits.  
- Dwell time `T` converted to `T_sec` â†’ `n_ticks` for event confirmation.

**Alerts & Errors Implementation (2 pt)**  
- Symmetric two-sided detection with dwell (ALERT/ERROR).  
- Event log CSV emitted with timestamps/axis/label.

**Visualization / Dashboard (0.5 pt)**  
- Clear overlays + decluttered summary PDF.  
- Playback utility with top-K and stacked options.

---

## 6) Module overview (OOP)

- **DataExtractionAnalysis.DataExplorer**  
  Source detection (DB/CSV), schema/health checks, TRAIN vs TEST overlays.

- **DataPreparation.DataPreparer**  
  Time standardization (`Time` â†’ `t_sec`), axis policy, cleaning, upload CSVâ†’DB, cadence/gap verification, EDA summary export.

- **ModelSelection.ModelSelector**  
  Align TESTâ†’TRAIN (affine), pick per-axis model (linear/mean), optional stats (Shapiro-Wilk where SciPy available).

- **ModelTraining.ModelTrainer**  
  Train/persist per-axis estimators, compute residuals, MAE logging.

- **ModelEvaluationValidation.ModelEvaluator**  
  IQR thresholds + dwell, event detection, overlay/summary plots, PDF report.

- **TrainedMLModel.ArtifactStore**  
  Save/load model bundles; thresholds with axis-index normalization; event CSV writer.

- **scripts.playback**  
  Sliding window stacked view with ALERT/ERROR spans; z-mode or current-mode; top-K declutter.

---

## 7) Configuration options

You can pass settings via **CLI flags**, **.env**, or **YAML** (`--config pipeline.yaml`).  
Priority: CLI > YAML > .env defaults.

Common flags:
- `--train_table`, `--test_csv`
- `--cadence_s`, `--iqr_k`, `--t_mult`
- `--plot {none, overlays, summary, both}`
- `--artifacts` (project root; default `.` recommended)

---

## 8) Logging, artifacts & reports

- **Logs:** `logs/app.log` with rotating handlers.  
- **Artifacts:** `artifacts/models/*.joblib`, `artifacts/thresholds.csv`, `artifacts/events_*.csv`.  
- **Reports/Plots:** `reports/summary.pdf` (+ individual PNGs); key outputs also copied to `artifacts/`.

> Optional: if `MLFLOW_TRACKING_URI` is set, runs log MAE and artifacts to MLflow.

---

## 9) Known tweaks / fixes applied

1) **Playback artifact root:** default should be **`.`** (project root) so loaders resolve `./artifacts/...`. If not yet changed in code, pass `--artifacts .`.  
2) **EDA report pathing:** `DataPreparer.eda_report()` should import and use `from pathlib import Path` (avoid `pd.Path`, which doesnâ€™t exist).

---

## 10) Troubleshooting

- **ModuleNotFoundError / Import issues**  
  Activate the virtualenv, then `pip install -r requirements.txt`.

- **DB connection fails**  
  Verify `DATABASE_URL` in `.env`. Test with a simple `psql` or `sqlalchemy.create_engine(...).connect()`.

- **Playback shows file-not-found**  
  Ensure you trained at least once (so `artifacts/` exists) and pass `--artifacts .`.

- **Plots look cluttered**  
  Use playback flags: `--topk 4`, `--stacked 1`, and an appropriate `--window` (e.g., 45â€“90s).

---

## 11) Suggested submission notes (for Professor)

- **Architecture clarity:** Modules match the provided boundaries; single CLI entry point.  
- **Reproducibility:** .env + optional YAML config; artifacts versioned in `artifacts/`.  
- **Evidence:** Generated `reports/summary.pdf` contains overlays, residuals, and event windows.  
- **Streaming parity:** Playback replicates notebookâ€™s final visualization with dwell logic.

---

## 12) License

Academic / educational use for course submission. Add your preferred license if publishing publicly.

---

### One-liners (handy)

```powershell
# Clean + re-create venv (Windows)
Remove-Item .venv -Recurse -Force; py -m venv .venv; .\.venv\Scripts\Activate.ps1; pip install -r requirements.txt
```

```bash
# Clean + re-create venv (macOS/Linux)
rm -rf .venv && python3 -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt
```

---

## Appendix: pipeline.yaml (reference)

Place this file at the repo root (alongside README.md) to run with `--config pipeline.yaml`.

```yaml
# Pipeline defaults for Predictive Maintenance Orchestration (Lab 1 Refactor)

# --- Data Sources ---
train_table: readings_fact_csv        # TRAIN table name in DB (from .env DATABASE_URL)
test_csv: data/raw/metadata_wilk_clean.csv

# --- Data Prep ---
cadence_s: 1.0                        # expected sample cadence in seconds
axis_policy: all                      # axes to include; e.g., "topk:4" or "all"

# --- Model Selection ---
model_type: auto                      # auto = choose best per axis (linear/mean)
similarity_check: true                # align TESTâ†’TRAIN (affine)

# --- Threshold Discovery ---
iqr_k: 1.5                            # IQR multiplier (robust minC/maxC)
t_mult: 3                             # dwell multiplier â†’ T_sec

# --- Visualization ---
plot: both                            # {none, overlays, summary, both}
overlay_topk: 8                       # show top-K axes in overlay
summary_decimate: 3                   # declutter factor (every Nth point)
summary_smooth_win: 5                 # rolling median window for plots

# --- Artifacts & Reporting ---
artifacts_dir: artifacts              # where models/thresholds are saved
reports_dir: reports                  # where PDF summaries go
log_dir: logs                         # rotating logs

# --- Playback defaults (optional) ---
playback:
  tick: 2                             # seconds per step
  window: 45                          # sliding window size (sec)
  stacked: 1                          # stacked view (1=true)
  zmode: 0                            # 0=current-mode, 1=z-mode
  topk: 8                             # noisiest axes to show

# --- Optional MLflow logging ---
mlflow:
  enabled: false
  tracking_uri: ""                    # e.g., http://127.0.0.1:5000
  experiment_name: PredMaint_Lab1
```

---

## Submission Notes

- **Course:** CAS Applied AI & Machine Learning, Conestoga College  
- **Lab:** Lab 1 â€“ Predictive Maintenance Orchestration Refactor  
- **Student:** Sabrina Ronnie George Karippatt (ID: 8991911)  
- **Date:** Sept 27, 2025  

---
